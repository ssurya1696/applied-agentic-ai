{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eab34414",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import uuid\n",
    "import logging\n",
    "from typing import Dict\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32dafedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "OPENAI_API_KEY = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ea3a5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRIMARY_MODEL = \"gpt-5-nano\"   # Default model\n",
    "FALLBACK_MODEL = \"gpt-4o-mini\"\n",
    "TEMPERATURE = 0\n",
    "MAX_RETRIES = 2\n",
    "\n",
    "MODEL_PRICING = {\n",
    "    \"gpt-5-nano\": {\"input\": 0.0005, \"output\": 0.001},\n",
    "    \"gpt-4o-mini\": {\"input\": 0.001, \"output\": 0.003},\n",
    "}\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea9cea6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_llm = ChatOpenAI(\n",
    "    model=PRIMARY_MODEL,\n",
    "    temperature=TEMPERATURE,\n",
    "    api_key=OPENAI_API_KEY\n",
    ")\n",
    "\n",
    "fallback_llm = ChatOpenAI(\n",
    "    model=FALLBACK_MODEL,\n",
    "    temperature=TEMPERATURE,\n",
    "    api_key=OPENAI_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "608e640e",
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS = {\n",
    "    \"total_requests\": 0,\n",
    "    \"total_cost\": 0.0,\n",
    "    \"total_tokens\": 0,\n",
    "    \"fallback_count\": 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3740f46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cost(model: str, input_tokens: int, output_tokens: int) -> float:\n",
    "    pricing = MODEL_PRICING.get(model)\n",
    "    if not pricing:\n",
    "        return 0.0\n",
    "\n",
    "    cost = (\n",
    "        input_tokens * pricing[\"input\"] +\n",
    "        output_tokens * pricing[\"output\"]\n",
    "    ) / 1000\n",
    "\n",
    "    return round(cost, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2689899",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_llm(llm, model_name: str, prompt: str, trace_id: str):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    response = llm.invoke([HumanMessage(content=prompt)])\n",
    "\n",
    "    latency = round(time.time() - start_time, 3)\n",
    "\n",
    "    # Extract token usage (LangChain stores in response_metadata)\n",
    "    usage = response.response_metadata.get(\"token_usage\", {})\n",
    "\n",
    "    input_tokens = usage.get(\"prompt_tokens\", 0)\n",
    "    output_tokens = usage.get(\"completion_tokens\", 0)\n",
    "\n",
    "    cost = calculate_cost(model_name, input_tokens, output_tokens)\n",
    "\n",
    "    # Update metrics\n",
    "    METRICS[\"total_requests\"] += 1\n",
    "    METRICS[\"total_cost\"] += cost\n",
    "    METRICS[\"total_tokens\"] += (input_tokens + output_tokens)\n",
    "\n",
    "    logging.info({\n",
    "        \"trace_id\": trace_id,\n",
    "        \"model\": model_name,\n",
    "        \"latency\": latency,\n",
    "        \"input_tokens\": input_tokens,\n",
    "        \"output_tokens\": output_tokens,\n",
    "        \"cost\": cost\n",
    "    })\n",
    "\n",
    "    return {\n",
    "        \"trace_id\": trace_id,\n",
    "        \"model\": model_name,\n",
    "        \"output\": response.content,\n",
    "        \"latency\": latency,\n",
    "        \"input_tokens\": input_tokens,\n",
    "        \"output_tokens\": output_tokens,\n",
    "        \"cost\": cost\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef493ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_with_retry(llm, model_name, prompt, trace_id):\n",
    "\n",
    "    for attempt in range(MAX_RETRIES + 1):\n",
    "        try:\n",
    "            return call_llm(llm, model_name, prompt, trace_id)\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error({\n",
    "                \"trace_id\": trace_id,\n",
    "                \"attempt\": attempt,\n",
    "                \"error\": str(e)\n",
    "            })\n",
    "\n",
    "            if attempt == MAX_RETRIES:\n",
    "                raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b72ee93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_orchestrator(prompt: str):\n",
    "\n",
    "    trace_id = str(uuid.uuid4())\n",
    "\n",
    "    try:\n",
    "        result = execute_with_retry(\n",
    "            primary_llm,\n",
    "            PRIMARY_MODEL,\n",
    "            prompt,\n",
    "            trace_id\n",
    "        )\n",
    "\n",
    "    except Exception:\n",
    "        METRICS[\"fallback_count\"] += 1\n",
    "\n",
    "        result = call_llm(\n",
    "            fallback_llm,\n",
    "            FALLBACK_MODEL,\n",
    "            prompt,\n",
    "            trace_id\n",
    "        )\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "033643b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:{'trace_id': 'a20eaed1-f60f-467f-8154-4886a188601c', 'model': 'gpt-5-nano', 'latency': 20.784, 'input_tokens': 15, 'output_tokens': 2059, 'cost': 0.002067}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'trace_id': 'a20eaed1-f60f-467f-8154-4886a188601c', 'model': 'gpt-5-nano', 'output': 'Fallback mechanisms for LLMs preserve availability, reliability, and response quality when primary models falter. Techniques include multiâ€‘model orchestration, where requests route to a smaller or local model if the cloud API fails or latency spikes. Caching and retrieval-augmented generation reduce repeated latency by reusing answers or precomputed spans. Timeouts and circuit breakers trigger safe defaults and prevent cascading outages. Graceful degradation can answer with templates or rule-based heuristics when generation is unavailable. Monitoring, retries, and rate control adjust traffic, while auditing keeps privacy and safety in check. Documentation informs users about limits and fallback behavior, across platforms and teams globally.', 'latency': 20.784, 'input_tokens': 15, 'output_tokens': 2059, 'cost': 0.002067}\n"
     ]
    }
   ],
   "source": [
    "response = run_orchestrator(\"Explain llm fallback mechanisms in 100 words\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9896f383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== REQUEST SUMMARY =====\n",
      "Trace ID: a20eaed1-f60f-467f-8154-4886a188601c\n",
      "Model Used: gpt-5-nano\n",
      "Latency (s): 20.784\n",
      "Input Tokens: 15\n",
      "Output Tokens: 2059\n",
      "Cost ($): 0.002067\n",
      "\n",
      "===== SYSTEM METRICS =====\n",
      "Total Requests: 1\n",
      "Total Tokens: 2074\n",
      "Total Cost ($): 0.002067\n",
      "Fallback Triggered: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"===== REQUEST SUMMARY =====\")\n",
    "print(\"Trace ID:\", response[\"trace_id\"])\n",
    "print(\"Model Used:\", response[\"model\"])\n",
    "print(\"Latency (s):\", response[\"latency\"])\n",
    "print(\"Input Tokens:\", response[\"input_tokens\"])\n",
    "print(\"Output Tokens:\", response[\"output_tokens\"])\n",
    "print(\"Cost ($):\", response[\"cost\"])\n",
    "\n",
    "print(\"\\n===== SYSTEM METRICS =====\")\n",
    "print(\"Total Requests:\", METRICS[\"total_requests\"])\n",
    "print(\"Total Tokens:\", METRICS[\"total_tokens\"])\n",
    "print(\"Total Cost ($):\", round(METRICS[\"total_cost\"], 6))\n",
    "print(\"Fallback Triggered:\", METRICS[\"fallback_count\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
